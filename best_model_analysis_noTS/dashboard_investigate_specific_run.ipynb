{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import HBox, VBox\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import pearsonr\n",
    "from IPython.display import display, clear_output\n",
    "import utils_plots as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dictiona\n",
    "n_clusters_dict = {'NEP': 9, 'NWP': 8, 'NA': 12, 'NI': 9, 'SI': 10, 'SP': 11}\n",
    "\n",
    "run_name_dict = {\n",
    "    'NEP': [f'test60_linreg_nc{n_clusters_dict[\"NEP\"]}_nv8_nd9_noTS',\n",
    "            f'test78_linreg_nc{n_clusters_dict[\"NEP\"]}_nv8_nd9_noTS',\n",
    "            f'test87_linreg_nc{n_clusters_dict[\"NEP\"]}_nv8_nd9_noTS',\n",
    "            f'selfeat50_top20_nc{n_clusters_dict[\"NEP\"]}_nv8_nd9_noTS',\n",
    "            f'selfeat60_top20_nc{n_clusters_dict[\"NEP\"]}_nv8_nd9_noTS',\n",
    "            f'selfeat70_top20_nc{n_clusters_dict[\"NEP\"]}_nv8_nd9_noTS',\n",
    "            f'selfeat75_top20_nc{n_clusters_dict[\"NEP\"]}_nv8_nd9_noTS',\n",
    "            f'selfeat80_top20_nc{n_clusters_dict[\"NEP\"]}_nv8_nd9_noTS',\n",
    "            f'selfeat90_top20_nc{n_clusters_dict[\"NEP\"]}_nv8_nd9_noTS'],\n",
    "    'NWP': [f'test4_linreg_nc{n_clusters_dict[\"NWP\"]}_nv8_nd9_noTS',\n",
    "            f'test25_linreg_nc{n_clusters_dict[\"NWP\"]}_nv8_nd9_noTS',\n",
    "            f'test83_linreg_nc{n_clusters_dict[\"NWP\"]}_nv8_nd9_noTS',\n",
    "            f'selfeat50_top20_nc{n_clusters_dict[\"NWP\"]}_nv8_nd9_noTS',\n",
    "            f'selfeat60_top20_nc{n_clusters_dict[\"NWP\"]}_nv8_nd9_noTS',\n",
    "            f'selfeat70_top20_nc{n_clusters_dict[\"NWP\"]}_nv8_nd9_noTS',\n",
    "            f'selfeat75_top20_nc{n_clusters_dict[\"NWP\"]}_nv8_nd9_noTS',\n",
    "            f'selfeat80_top20_nc{n_clusters_dict[\"NWP\"]}_nv8_nd9_noTS',\n",
    "            f'selfeat90_top20_nc{n_clusters_dict[\"NWP\"]}_nv8_nd9_noTS'],\n",
    "    'NA': [f'test3_linreg_nc{n_clusters_dict[\"NA\"]}_nv8_nd9_noTS',\n",
    "           f'test14_linreg_nc{n_clusters_dict[\"NA\"]}_nv8_nd9_noTS',\n",
    "           f'test61_linreg_nc{n_clusters_dict[\"NA\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat50_top20_nc{n_clusters_dict[\"NA\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat60_top20_nc{n_clusters_dict[\"NA\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat70_top20_nc{n_clusters_dict[\"NA\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat75_top20_nc{n_clusters_dict[\"NA\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat80_top20_nc{n_clusters_dict[\"NA\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat90_top20_nc{n_clusters_dict[\"NA\"]}_nv8_nd9_noTS'],\n",
    "    'NI': [f'test26_linreg_nc{n_clusters_dict[\"NI\"]}_nv8_nd9_noTS',\n",
    "           f'test32_linreg_nc{n_clusters_dict[\"NI\"]}_nv8_nd9_noTS',\n",
    "           f'test45_linreg_nc{n_clusters_dict[\"NI\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat50_top20_nc{n_clusters_dict[\"NI\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat60_top20_nc{n_clusters_dict[\"NI\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat70_top20_nc{n_clusters_dict[\"NI\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat75_top20_nc{n_clusters_dict[\"NI\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat80_top20_nc{n_clusters_dict[\"NI\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat90_top20_nc{n_clusters_dict[\"NI\"]}_nv8_nd9_noTS'],\n",
    "    'SI': [f'test12_linreg_nc{n_clusters_dict[\"SI\"]}_nv8_nd9_noTS',\n",
    "           f'test51_linreg_nc{n_clusters_dict[\"SI\"]}_nv8_nd9_noTS',\n",
    "           f'test82_linreg_nc{n_clusters_dict[\"SI\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat50_top20_nc{n_clusters_dict[\"SI\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat60_top20_nc{n_clusters_dict[\"SI\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat70_top20_nc{n_clusters_dict[\"SI\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat75_top20_nc{n_clusters_dict[\"SI\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat80_top20_nc{n_clusters_dict[\"SI\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat90_top20_nc{n_clusters_dict[\"SI\"]}_nv8_nd9_noTS'],\n",
    "    'SP': [f'test8_linreg_nc{n_clusters_dict[\"SP\"]}_nv8_nd9_noTS',\n",
    "           f'test23_linreg_nc{n_clusters_dict[\"SP\"]}_nv8_nd9_noTS',\n",
    "           f'test98_linreg_nc{n_clusters_dict[\"SP\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat50_top20_nc{n_clusters_dict[\"SP\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat60_top20_nc{n_clusters_dict[\"SP\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat70_top20_nc{n_clusters_dict[\"SP\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat75_top20_nc{n_clusters_dict[\"SP\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat80_top20_nc{n_clusters_dict[\"SP\"]}_nv8_nd9_noTS',\n",
    "           f'selfeat90_top20_nc{n_clusters_dict[\"SP\"]}_nv8_nd9_noTS'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bbf44d2c29a477c914b92020bda1b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='<b>Basin:</b>', layout=Layout(width='170px')), Dropdown(layout=Layout(width='220px'…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0d2d13e166f4d38bcb0cf08f5189124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='<b>Run name:</b>', layout=Layout(width='170px')), Dropdown(layout=Layout(width='220…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define widgets\n",
    "widget_layout1 = widgets.Layout(width='300px')\n",
    "widget_layout2 = widgets.Layout(width='170px')\n",
    "\n",
    "basin_widget = widgets.Dropdown(\n",
    "    options=['NEP', 'NWP', 'NA', 'NI', 'SP', 'SI'],\n",
    "    value='NEP',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widget_layout1,\n",
    ")\n",
    "label_basin_widget = widgets.HTML(value=\"<b>Basin:</b>\", layout=widget_layout2)\n",
    "basin_box = HBox([label_basin_widget, basin_widget])\n",
    "\n",
    "run_name_widget = widgets.Dropdown(\n",
    "    options=run_name_dict[basin_widget.value],  # initialized with first basin\n",
    "    value=run_name_dict[basin_widget.value][0],\n",
    "    layout=widget_layout1,\n",
    ")\n",
    "label_run_widget = widgets.HTML(value=\"<b>Run name:</b>\", layout=widget_layout2)\n",
    "run_box = HBox([label_run_widget, run_name_widget])\n",
    "\n",
    "update_button = widgets.Button(\n",
    "    description = 'Update',\n",
    "    icon = \"check\",\n",
    "    layout=widgets.Layout(height='auto'),\n",
    "    style={'button_color': 'lightgreen'}\n",
    ")\n",
    "\n",
    "# Link Basin -> Run options\n",
    "def update_run_name_options(change):\n",
    "    new_basin = change['new']\n",
    "    run_options = run_name_dict[new_basin]\n",
    "    run_name_widget.options = run_options\n",
    "    run_name_widget.value = run_options[0]\n",
    "\n",
    "basin_widget.observe(update_run_name_options, names='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_output(basin, run_name):\n",
    "    # Number of clusters \n",
    "    n_clusters_dict = {'NEP': 9, 'NWP': 8, 'NA': 12, 'NI': 9, 'SI': 10, 'SP': 11}\n",
    "    n_clusters = n_clusters_dict[basin]\n",
    "    # Set years range and number of folds\n",
    "    years = np.arange(1980, 2022, 1) # from 1980 to 2021 included\n",
    "    n_folds = 3\n",
    "    # Set directories and file paths, then load file containing predictors and target\n",
    "    project_dir = '/Users/huripari/Documents/PhD/TCs_Genesis'\n",
    "    fs_dir = os.path.join(project_dir, 'xai-gpi')\n",
    "    cluster_data = f'{basin}_{n_clusters}clusters_noTS'\n",
    "    cluster_data_dir = os.path.join(fs_dir, 'data', cluster_data)\n",
    "    # predictors\n",
    "    predictor_file = f'predictors_1980-2022_{n_clusters}clusters_8vars_9idxs.csv'\n",
    "    predictors_df = pd.read_csv(os.path.join(cluster_data_dir, predictor_file), index_col=0)\n",
    "    predictors_df.index = pd.to_datetime(predictors_df.index)\n",
    "    predictors_df = predictors_df.loc[predictors_df.index.year.isin(years)]\n",
    "    # target\n",
    "    target_file = 'target_residual_1980-2022_2.5x2.5.csv'\n",
    "    seasonal_file = 'target_seasonality_1980-2022_2.5x2.5.csv'\n",
    "    trend_file = 'target_trend_1980-2022_2.5x2.5.csv'\n",
    "    target_df = pd.read_csv(os.path.join(cluster_data_dir, target_file), index_col=0)\n",
    "    target_df.index = pd.to_datetime(target_df.index)\n",
    "    target_df = target_df.loc[target_df.index.year.isin(years)]\n",
    "    target_season_df = pd.read_csv(os.path.join(cluster_data_dir, seasonal_file), index_col=0)\n",
    "    target_season_df.index = pd.to_datetime(target_season_df.index)\n",
    "    target_season_df = target_season_df.loc[target_season_df.index.year.isin(years)]\n",
    "    target_trend_df = pd.read_csv(os.path.join(cluster_data_dir, trend_file), index_col=0)\n",
    "    target_trend_df.index = pd.to_datetime(target_trend_df.index)\n",
    "    target_trend_df = target_trend_df.loc[target_trend_df.index.year.isin(years)]\n",
    "    # gpis\n",
    "    gpis_file = f'{basin}_2.5x2.5_gpis_time_series.csv'\n",
    "    gpis_path = os.path.join(fs_dir, 'data', gpis_file)\n",
    "    gpis_df = pd.read_csv(gpis_path, index_col=0)\n",
    "    gpis_df.index = pd.to_datetime(gpis_df.index)\n",
    "    gpis_df = gpis_df.loc[gpis_df.index.year.isin(years)]\n",
    "    # Get the run info and data\n",
    "    Y_pred, Y_pred_noFS, X_test_eval, X_test_eval_noFS, mlps, mlps_noFS, perm_importance_mlp, perm_importance_mlp_noFS, shap_values_mlp, shap_values_mlp_noFS = ut.runs_info(basin, run_name)\n",
    "    # Convert list of dataframes to a single dataframe\n",
    "    X_test = pd.concat(X_test_eval)\n",
    "    X_test_noFS = pd.concat(X_test_eval_noFS)\n",
    "    Y_pred_df = pd.concat(Y_pred)\n",
    "    Y_pred_noFS_df = pd.concat(Y_pred_noFS)\n",
    "    ## Time series Trajectories and Metrics ##\n",
    "    # Predictions with trend and seasonality\n",
    "    Y_pred_df_TS = Y_pred_df['resid'] + target_trend_df['trend'] + target_season_df['season']\n",
    "    Y_pred_noFS_df_TS = Y_pred_noFS_df['resid'] + target_trend_df['trend'] + target_season_df['season']\n",
    "    Y_pred_df_TS[Y_pred_df_TS < 0] = 0.0\n",
    "    Y_pred_noFS_df_TS[Y_pred_noFS_df_TS < 0] = 0.0\n",
    "    # Annual data without trend and seasonality\n",
    "    target_df_annual = target_df.groupby(target_df.index.year).sum()\n",
    "    Y_pred_df_annual = Y_pred_df.groupby(Y_pred_df.index.year).sum()\n",
    "    Y_pred_noFS_df_annual = Y_pred_noFS_df.groupby(Y_pred_noFS_df.index.year).sum()\n",
    "    # GPIs time series with trend and seasonality\n",
    "    engpi_TS = gpis_df['engpi']\n",
    "    ogpi_TS = gpis_df['ogpi']\n",
    "    # GPIs time series without trend and seasonality\n",
    "    decomp_engpi = STL(engpi_TS).fit()\n",
    "    trend_engpi = decomp_engpi.trend\n",
    "    seasonal_engpi = decomp_engpi.seasonal\n",
    "    engpi = decomp_engpi.resid\n",
    "    decomp_ogpi = STL(ogpi_TS).fit()\n",
    "    trend_ogpi = decomp_ogpi.trend\n",
    "    seasonal_ogpi = decomp_ogpi.seasonal\n",
    "    ogpi = decomp_ogpi.resid\n",
    "    # Annual data of the GPIs\n",
    "    engpi_annual = engpi.groupby(engpi.index.year).sum()\n",
    "    ogpi_annual = ogpi.groupby(ogpi.index.year).sum()\n",
    "    # Compute the correlation coefficient and the MSE between the predictions and the test values\n",
    "    # Monthly without trend and seasonality\n",
    "    r, _ = pearsonr(target_df['resid'], Y_pred_df['resid'])\n",
    "    r_noFS, _ = pearsonr(target_df['resid'], Y_pred_noFS_df['resid'])\n",
    "    r_engpi, _ = pearsonr(target_df['resid'], engpi)\n",
    "    r_ogpi, _ = pearsonr(target_df['resid'], ogpi)\n",
    "    # Annual without trend and seasonality\n",
    "    rY, _ = pearsonr(target_df_annual['resid'], Y_pred_df_annual['resid'])\n",
    "    rY_noFS, _ = pearsonr(target_df_annual['resid'], Y_pred_noFS_df_annual['resid'])\n",
    "    rY_engpi, _ = pearsonr(target_df_annual['resid'], engpi_annual)\n",
    "    rY_ogpi, _ = pearsonr(target_df_annual['resid'], ogpi_annual)\n",
    "    # Plotting the monthly time series detrended and deseasonalized\n",
    "    fig_ts = ut.plot_monthly_time_series(target_df['resid'], Y_pred_df['resid'], Y_pred_noFS_df['resid'], engpi, ogpi, r, r_noFS, r_engpi, r_ogpi)\n",
    "    # Plotting the annual time series detrended and deseasonalized\n",
    "    fig_annual = ut.plot_annual_time_series(target_df_annual['resid'], Y_pred_df_annual['resid'], Y_pred_noFS_df_annual['resid'], engpi_annual, ogpi_annual, rY, rY_noFS, rY_engpi, rY_ogpi)\n",
    "    ## Selected features ##\n",
    "    # Determine selected features according to the run_name\n",
    "    if 'selfeat' in run_name:\n",
    "        perc = run_name.split('_top20')[0].split('selfeat')[1]\n",
    "        csv_path = os.path.join(fs_dir, 'results', f'selected_features_best_models_{basin}_{n_clusters}_noTS.csv')\n",
    "        df_perc_sel = pd.read_csv(csv_path, index_col=0)\n",
    "        selected_features = df_perc_sel[str(perc)].dropna().to_list()\n",
    "    elif 'test' in run_name:\n",
    "        experiment_filename = f'1980-2022_{n_clusters}clusters_8vars_9idxs.csv'\n",
    "        sol_filename = 'linreg_' + experiment_filename\n",
    "        output_dir = os.path.join(fs_dir, 'results', basin, run_name)\n",
    "        best_sol_path = os.path.join(output_dir, f'best_solution_{sol_filename}')\n",
    "        best_solution = pd.read_csv(best_sol_path, sep=',', header=None)\n",
    "        best_solution = best_solution.to_numpy().flatten()\n",
    "        column_names = predictors_df.columns.tolist()\n",
    "        final_sequence = best_solution[len(column_names):2*len(column_names)]\n",
    "        sequence_length = best_solution[:len(column_names)]\n",
    "        feat_sel = best_solution[2*len(column_names):]\n",
    "        variable_selection = feat_sel.astype(int)\n",
    "        time_sequences = sequence_length.astype(int)\n",
    "        time_lags = final_sequence.astype(int)\n",
    "        selected_features = []\n",
    "        for c, col in enumerate(predictors_df.columns):\n",
    "            if variable_selection[c] == 0 or time_sequences[c] == 0:\n",
    "                continue\n",
    "            for j in range(time_sequences[c]):\n",
    "                selected_features.append(str(col))\n",
    "    else:\n",
    "        raise ValueError(f'Unknown run name: {run_name}')\n",
    "    # Get the variables names and the selected clusters\n",
    "    variables_with_cluster = [var for var in selected_features if 'cluster' in var]\n",
    "    variables_without_cluster = [var for var in selected_features if 'cluster' not in var]\n",
    "    variable_names_cluster = [var.split('_cluster')[0] for var in variables_with_cluster]\n",
    "    variable_names_cluster = list(set(variable_names_cluster))\n",
    "    variable_names_cluster.sort()\n",
    "    # Plot the selected features\n",
    "    fig_clusters = ut.plot_variables_clusters(basin, n_clusters, cluster_data_dir, variable_names_cluster, selected_features)\n",
    "    ## SHAP values ##\n",
    "    years_couples = []\n",
    "    # Create a DataFrame with fold number corresponding to each year and also the couple of max and min years for each fold\n",
    "    kfold = KFold(n_splits=n_folds)\n",
    "    test_years_df = pd.DataFrame(0, index=years, columns=['fold'])\n",
    "    for nf, (train_index, test_index) in enumerate(kfold.split(years)):\n",
    "        test_years_df.loc[years[test_index], 'fold'] = nf\n",
    "        Y_pred_df_annual_fold = Y_pred_df_annual.loc[years[test_index]]\n",
    "        max_fold = Y_pred_df_annual_fold['resid'].idxmax()\n",
    "        min_fold = Y_pred_df_annual_fold['resid'].idxmin()\n",
    "        years_couples.append((max_fold, min_fold))\n",
    "    # Plot shap values for each fold\n",
    "    fig_shap = ut.plot_shap_values(shap_values_mlp)\n",
    "    # Plot shap values with min max years\n",
    "    fig_shap_minmax = ut.plot_minmax_shap_values(shap_values_mlp, years_couples, Y_pred, test_years_df)\n",
    "\n",
    "# Merge layout of the widgets\n",
    "widgets_layout = HBox([VBox([basin_box, run_box]), update_button])\n",
    "display(widgets_layout)\n",
    "# Update plot based on the selected basin and run name\n",
    "outputs = widgets.Output()\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with outputs:\n",
    "        clear_output(wait=True)\n",
    "        update_output(basin_widget.value, run_name_widget.value)\n",
    "\n",
    "update_button.on_click(on_button_clicked)\n",
    "display(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "weather2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
